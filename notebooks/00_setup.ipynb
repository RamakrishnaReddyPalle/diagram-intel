{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05712d7",
   "metadata": {},
   "source": [
    "# **1) Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7cc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys, platform, json, subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e6afdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\IIT BBS\\Job Resources\\fiolabs\\diagram-intel\n",
      "Python: 3.11.13 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:03:15) [MSC v.1929 64 bit (AMD64)]\n",
      "OS: Windows-10-10.0.26100-SP0\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "\n",
    "if (ROOT / \"src\").exists():\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"Project root:\", ROOT)\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OS:\", platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6290326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env exists: False  | .env.example: True\n",
      "configs/paths.yaml exists: True\n",
      "configs/base.yaml exists: True\n",
      "configs/pipeline.yaml exists: True\n",
      "configs/models.yaml exists: True\n"
     ]
    }
   ],
   "source": [
    "# .env and configs reachability\n",
    "print(\".env exists:\", (ROOT/\".env\").exists(), \" | .env.example:\", (ROOT/\".env.example\").exists())\n",
    "for f in [\"configs/paths.yaml\",\"configs/base.yaml\",\"configs/pipeline.yaml\",\"configs/models.yaml\"]:\n",
    "    p = ROOT / f\n",
    "    print(f\"{f} exists:\", p.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85ab160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdftocairo OK: False | no output\n"
     ]
    }
   ],
   "source": [
    "# Poppler (pdftocairo) check\n",
    "import subprocess, shutil\n",
    "\n",
    "try:\n",
    "    out = subprocess.run([\"pdftocairo\", \"-v\"], capture_output=True, text=True)\n",
    "    msg = (out.stderr or out.stdout or \"\").splitlines()\n",
    "    first_line = msg[0] if msg else \"no output\"\n",
    "    print(\"pdftocairo OK:\", out.returncode==0, \"|\", first_line)\n",
    "except FileNotFoundError:\n",
    "    print(\"pdftocairo NOT FOUND. If using conda: conda install -c conda-forge poppler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0b83b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\diagram-intel\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cpu\n",
      "Transformers: 4.55.4\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Torch/Transformers + CUDA\n",
    "import torch, transformers\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device 0:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ed9b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths:\n",
      "  data_root: ./data\n",
      "  input_pdfs: ${paths.data_root}/input_pdfs\n",
      "  raw: ${paths.data_root}/raw\n",
      "  interim: ${paths.data_root}/interim\n",
      "  processed: ${paths.data_root}/processed\n",
      "  exports: ${paths.data_root}/exports\n",
      "  model_cache: ./models/cache\n",
      "runtime:\n",
      "  device: ${env:DEVICE, \"cpu\"}\n",
      "  precision: ${env:PRECISION, \"float16\"}\n",
      "  workers: 4\n",
      "  seed: 42\n",
      "  dpi: 900\n",
      "  tile:\n",
      "    micro_size: 512\n",
      "    meso_size: 1024\n",
      "    macro_size: 2048\n",
      "    overlap: 0.15\n",
      "logging:\n",
      "  level: ${env:LOG_LEVEL, \"INFO\"}\n",
      "phase ...\n",
      "\n",
      "registry.json exists: True\n",
      "{\n",
      "  \"qwen2_vl\": {\n",
      "    \"repo_id\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
      "    \"local_path\": \"D:\\\\IIT BBS\\\\Job Resources\\\\fiolabs\\\\diagram-intel\\\\models\\\\cache\\\\Qwen2-VL-7B-Instruct\",\n",
      "    \"status\": \"present\"\n",
      "  },\n",
      "  \"qwen2_vl_2b\": {\n",
      "    \"repo_id\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "    \"local_path\": \"D:\\\\IIT BBS\\\\Job Resources\\\\fiolabs\\\\diagram-intel\\\\models\\\\cache\\\\Qwen2-VL-2B-Instruct\",\n",
      "    \"status\": \"present\"\n",
      "  },\n",
      "  \"llava_v16_mistral_7b\": {\n",
      "    \"repo_id\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
      "    \"local_path\": \"D:\\\\IIT BBS\\\\Job Resources\\\\fiolabs\\\\diagram-intel\\\\models\\\\cache\\\\llava-v1.6-mistral-7b-hf\",\n",
      "    \"status\": \"present\"\n",
      "  },\n",
      "  \"donut\": {\n",
      "    \"repo_id\": \"naver-clova-ix/donut-base\",\n",
      "    \"local_path\": \"D:\\\\IIT BBS\\\\Job Resources\\\\fiolabs\\\\diagram-intel\\\\models\\\\cache\\\\donut-base\",\n",
      "    \"status\": \"presen ...\n"
     ]
    }
   ],
   "source": [
    "# Merged config + registry\n",
    "from src.config.loader import load_cfg\n",
    "from omegaconf import OmegaConf\n",
    "cfg = load_cfg()\n",
    "print(OmegaConf.to_yaml(cfg)[:500], \"...\\n\")\n",
    "\n",
    "reg_path = ROOT / \"models\" / \"registry.json\"\n",
    "print(\"registry.json exists:\", reg_path.exists())\n",
    "print(json.dumps(json.loads(reg_path.read_text()), indent=2)[:800], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2-VL sanity on CPU (robust \"{}\" parse from last line)\n",
    "\n",
    "from transformers import AutoProcessor\n",
    "import torch, json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def blank_image(w=64,h=64):\n",
    "    return Image.fromarray(np.ones((h,w,3), dtype=np.uint8)*255)\n",
    "\n",
    "def qwen_json_echo(local_path: str, max_new_tokens=64):\n",
    "    device = \"cpu\"\n",
    "    dtype  = torch.float32\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(local_path, trust_remote_code=True)\n",
    "\n",
    "    # Prefer new class; else fall back gracefully.\n",
    "    model = None\n",
    "    try:\n",
    "        from transformers import AutoModelForImageTextToText\n",
    "        model = AutoModelForImageTextToText.from_pretrained(\n",
    "            local_path, torch_dtype=dtype, device_map=None, trust_remote_code=True\n",
    "        )\n",
    "    except Exception:\n",
    "        try:\n",
    "            from transformers import AutoModelForVision2Seq\n",
    "            model = AutoModelForVision2Seq.from_pretrained(\n",
    "                local_path, torch_dtype=dtype, device_map=None, trust_remote_code=True\n",
    "            )\n",
    "        except Exception:\n",
    "            from transformers import Qwen2VLForConditionalGeneration\n",
    "            model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                local_path, torch_dtype=dtype, device_map=None, trust_remote_code=True\n",
    "            )\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    img = blank_image()\n",
    "    # Ask for a single-line JSON only.\n",
    "    prompt = \"Return only {} on a single line. Do not add any other text.\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt},\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    # 1) Get templated text (string)\n",
    "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    # 2) Tokenize with images\n",
    "    inputs = processor(text=[text], images=[img], return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "\n",
    "    out = processor.batch_decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    print(\"Full output tail:\\n\", out[-200:])\n",
    "\n",
    "    # Parse the **last** line that looks like JSON\n",
    "    for line in reversed(out.strip().splitlines()):\n",
    "        ls = line.strip()\n",
    "        if ls.startswith(\"{\") and ls.endswith(\"}\"):\n",
    "            try:\n",
    "                return json.loads(ls)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "    # Fallback: show full output for inspection\n",
    "    print(\"Could not find a clean JSON line. Full output:\\n\", out)\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49f99e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Qwen2-VL-2B @ D:\\IIT BBS\\Job Resources\\fiolabs\\diagram-intel\\models\\cache\\Qwen2-VL-2B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:40<00:00, 20.16s/it]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full output tail:\n",
      " system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Return only {} on a single line. Do not add any other text.\n",
      "assistant\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Run on 2B (lighter on CPU)\n",
    "import json\n",
    "reg = json.loads((ROOT/\"models/registry.json\").read_text())\n",
    "qwen2b_path = str(Path(reg[\"qwen2_vl_2b\"][\"local_path\"]))\n",
    "print(\"Testing Qwen2-VL-2B @\", qwen2b_path)\n",
    "print(qwen_json_echo(qwen2b_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac1ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diagram-intel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
